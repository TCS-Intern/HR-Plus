# David Kim, Ph.D.

**Machine Learning Engineer**

Seattle, WA | david.kim@email.com | +1-206-555-0404
[LinkedIn](https://linkedin.com/in/davidkim-ml) | [Google Scholar](https://scholar.google.com/davidkim)

---

## Summary

Machine Learning Engineer with 5 years of experience in NLP and computer vision. Published researcher with focus on large language models and production ML systems. Currently at OpenAI working on LLM training infrastructure.

---

## Experience

### Research Engineer
**OpenAI** | San Francisco, CA | January 2022 – Present

- Core contributor to GPT-4 training infrastructure, optimizing distributed training across 10k+ GPUs
- Built evaluation pipelines for model capability assessment and safety testing
- Developed data preprocessing systems handling 100TB+ of training data
- Published internal research on efficient fine-tuning methods
- Technologies: Python, PyTorch, CUDA, Ray, Kubernetes

### Machine Learning Engineer
**Meta (Facebook AI Research)** | Menlo Park, CA | September 2019 – December 2021

- Led NLP team building content understanding models serving 1B+ daily users
- Deployed multilingual text classification models with 95% accuracy across 50 languages
- Filed 3 patents on efficient inference techniques for transformer models
- Reduced model inference latency by 60% through quantization and distillation
- Technologies: Python, PyTorch, ONNX, Triton, C++

### Research Intern
**Google Brain** | Mountain View, CA | Summer 2018

- Developed novel attention mechanisms for sequence-to-sequence models
- Research contributed to published paper at NeurIPS 2018
- Technologies: TensorFlow, Python, TPU

---

## Education

### Carnegie Mellon University
**Ph.D. Machine Learning** | 2019 | GPA: 4.0/4.0
- Dissertation: "Efficient Training and Inference of Large Language Models"
- Advisor: Prof. Jane Smith
- Awards: Best Paper Award at ICML 2019

### Seoul National University
**B.S. Computer Science** | 2014 | GPA: 3.9/4.0
- Summa Cum Laude
- Presidential Science Scholarship

---

## Publications

1. **Kim, D.**, et al. (2023). "Scaling Laws for Efficient Fine-tuning of Large Language Models." *NeurIPS 2023*.

2. **Kim, D.**, et al. (2021). "Multi-task Learning for Multilingual NLP." *ACL 2021*. **Best Paper Award**.

3. **Kim, D.**, et al. (2019). "Attention Mechanisms for Sequence Modeling." *NeurIPS 2019*.

4. **Kim, D.**, et al. (2018). "Efficient Transformer Inference." *ICML 2018*.

---

## Skills

**ML Frameworks:** PyTorch (Expert), TensorFlow (Advanced), JAX (Intermediate), Hugging Face

**Languages:** Python (Expert), C++ (Advanced), CUDA (Advanced), SQL (Intermediate)

**ML Areas:** NLP, Transformers, LLMs, Computer Vision, Reinforcement Learning

**Infrastructure:** Kubernetes, Docker, Ray, AWS (SageMaker, EC2), GCP (Vertex AI)

**Tools:** Weights & Biases, MLflow, Git, Linux

---

## Patents

1. "Efficient Inference for Large Language Models" - US Patent Pending (2023)
2. "Multi-task Learning Architecture for NLP" - US Patent 11,234,567 (2022)
3. "Quantization Methods for Transformer Models" - US Patent Pending (2021)

---

## Talks & Presentations

- "Scaling LLMs: Lessons from GPT-4" - NeurIPS 2023 Workshop
- "Production ML at Scale" - MLOps Community Meetup 2022
- "Efficient Transformers" - PyTorch Developer Conference 2021
